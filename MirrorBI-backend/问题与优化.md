# 1、AI调用限流

[可以了，基于Redis和Lua实现分布式令牌桶限流 - 知乎](https://zhuanlan.zhihu.com/p/348030329)

为了**防止有用户恶意刷取AI调用，或者用户过多太多的请求导致服务器处理不了崩塌**，可以采用限流的手段进行优化。

> **限流解决了什么问题？**
>
> 保证了服务的**高可用性**，通过**牺牲一定的流量**，来提升系统的稳定性，防止大量流量服务器难以承受而崩溃。

> **限流带来了什么问题？**
>
> - 在服务层面，相当于多添加了一层中间件，引入了额外的开销，**增加了一次I/O环节，分布式限流还要走网络协议**，增加了响应延迟。
> - 引入了限流组件，添加了系统的复杂度，增加维护开销。
> - 限流组件拥有**流控权**，若限流组件崩溃，那么可能会引发**雪崩效应**，使用了该组件的**大部分请求都会失败。**

常见的限流方法有：

- **固定窗口计数**
  - 原理：将时间按照固定的时间间隔分片，对于每个用户，只允许在每一段时间内请求N次，当请求到达N次后就拒绝请求，直到下一个时间片。
  - 实现：**在内存**中维护一个Map，key为用户标识+窗口的起始时间，value为请求计数。或者使用Redis的`INCR`+`EXPIRE`
  - 优点：简单、高效。
  - 缺点：瞬时峰值可能很高，例如1min为一个时间片，用户在59s和1min01秒分别请求了N次，瞬时峰值来到2N。
- **滑动窗口计数**
  - 原理：为每一个请求记录时间戳，保留当前请求前一段时间（如60s）的所有请求时间戳，检查列表长度是否过长超出限制。
  - 实现：在内存或 **Redis 列表/有序集合**中 `LPUSH` 时间戳，然后 `LTRIM` 或者 `ZREMRANGEBYSCORE`。
  - 优点：精准控制任意连续时间内的请求数，没有边界效应。
  - 缺点：存储和遍历开销大。
- **漏桶算法**
  - 将请求看作水滴，灌入一个桶中。桶以恒定的速率“漏水”，进行对应的业务处理，桶满了则拒绝请求。
  - 实现：**维护一个队列来模拟桶**，每隔一段时间就去处理一个请求。
  - 优点：平滑请求输出，适合流量整形需求。
  - 缺点：**突发流量会有较大的排队延迟，对用户体验不佳**。并且因为需要固定速率处理，处理速率较慢，需要按顺序处理。
- **令牌桶算法（Token Bucket）**
  - 系统中心维护一个桶，**桶按照固定的速率产生令牌**，获取到令牌的请求才允许执行，否则需要等待或者拒绝。令牌桶需要有最大容量。
  - 实现：常见于 Linux 流量控制、Guava `RateLimiter`；分布式可用 Redis `INCR` + `EXPIRE` 结合脚本。
  - 优点：支持突发速率：**允许桶中积累令牌，用完后才会限速**。灵活平衡**“长期速率”与“短期峰值”**。
  - 缺点：实现较为复杂，需要计算令牌累积和消耗时机。时间单位选取需要仔细考虑。

# 2、优化Excel数据内嵌在Chart表的实践

在最初的设计方案中，每次导入的Excel数据会被解析成CSV文本数据，然后放置在`Chart`表中的`ChartData`字段。这会导致有以下的问题：

- 若Excel数据过大，那么一条记录就会占据大量的空间，降低查找效率。
- 每条记录都占据大多空间，一张表就会变得很庞大，对未来的增删差改都会带来很大的影响，造成系统的性能下降。

目前的设计是：

```go
type Chart struct {
	ID         uint64         `gorm:"primaryKey;comment:id" json:"id,string" swaggertype:"string"`
	Name       string         `gorm:"type:varchar(128);comment:图表名称" json:"name"`
	Goal       string         `gorm:"type:text;comment:分析目标" json:"goal"`
	ChartData  string         `gorm:"type:text;comment:图表数据" json:"chartData"`
	ChartType  string         `gorm:"type:varchar(128);comment:图表类型" json:"chartType"`
	GenChart   string         `gorm:"type:text;comment:AI生成的图表数据" json:"genChart"`
	GenResult  string         `gorm:"type:text;comment:AI生成的分析结论" json:"genResult"`
	UserID     uint64         `gorm:"comment:创建用户 id" json:"userId,string" swaggertype:"string"`
	CreateTime time.Time      `gorm:"autoCreateTime;comment:创建时间" json:"createTime"`
	UpdateTime time.Time      `gorm:"autoUpdateTime;comment:更新时间" json:"updateTime"`
	IsDelete   gorm.DeletedAt `gorm:"comment:是否删除" swaggerignore:"true" json:"isDelete"`
}
```

## 优化方案1、分表设计，将每个ChartData都分表为chart_{id}

这样是比较容易想出来的，但是带来的缺点也是明显的。**这样子会带来大量的DDL（Data Definition Language）管理和迁移的复杂度，每个图表都需要为之生成一个单独的表，不利于统一的查询和维护。**

## 优化方案2、使用**EAV（Entity-Attribute Value）通用表**

把所有的图表的单元格都放置到一张通用的表中，用`chart_id`来连接原始的表，表的结构抽象成如下格式：

```go
// ChartCell 对应 chart_data_cell 表
type ChartCell struct {
    ID        uint64 `gorm:"primaryKey"`
    ChartID   uint64 `gorm:"index;not null;comment:所属 Chart ID"`
    RowIndex  int    `gorm:"not null;comment:第几行，从0开始"`
    ColName   string `gorm:"type:varchar(128);not null;comment:列名，如 日期/测试/人数"`
    CellValue string `gorm:"type:text;comment:单元格内容"`
    // CreatedAt/UpdatedAt 可选
}
```

记录了行数、列名和列值。假如对于一个CSV例值：

```
日期,测试,人数
5.10号,,10
5.11号,123,20
5.12号,,30
```

那么它的存储结构为：

| chart_id | row_index | col_name | cell_value |
| -------- | --------- | -------- | ---------- |
| 1        | 0         | 日期     | 5.10号     |
| 1        | 0         | 测试     |            |
| 1        | 0         | 人数     | 10         |
| 1        | 1         | 日期     | 5.11号     |
| 1        | 1         | 测试     | 123        |
| 1        | 1         | 人数     | 20         |
| 1        | 2         | 日期     | 5.12号     |
| 1        | 2         | 测试     |            |
| 1        | 2         | 人数     | 30         |

优点：**较易于实现，可以实现行级的数据修改**。

缺点：每一行都需要一条单独的记录，表数据庞大的时候，**增大了内存的开销**。**想要重建成宽表，可能需要多次的JOIN操作，或者SELF-JOIN，I/O开销会很庞大。**可读性差，只能看见零散的格子。

## 落地方案、存储JSON字段

可以单独的提取一张表出来，然后存储的是CSV转化为JSON格式的数据。

```go
import "gorm.io/datatypes"

type ChartDataJSON struct {
    ChartID uint64         `gorm:"primaryKey;comment:图表ID"`
    Data    datatypes.JSON `gorm:"type:json;comment:二维表格数据"`  
}
```

优点：**最容易实现，每条记录都很聚集，查询效率高，直观可读。**查询也天然支持MySQL的JSON函数**做筛选或索引**。

缺点：**不适合做复杂的SQL查询。**

综合分析，当前网站的流量肯定不会很大，并且需要快速推进网站的建设，那么选择存储JSON字段的实现方式是合适的，高效的，收益大的。注意，不管使用哪种方案，**都需要引入事务来确保操作的原子性**。

# 3、开发完限流环节后，系统目前存在的体验问题

目前的AI分析执行流程为，当用户提交了分析需求后，需要等待服务器执行服务，阻塞地等待响应。如果离开页面，虽然结果不会丢失，但是会导致缺少响应的通知、让用户增加了步骤。并且，如果有**大量的用户的请求被阻塞在服务器等待执行**，也会导致系统承受不住压力而崩塌。

为了优化用户的体验，需要开发**异步处理任务**，让用户提交任务后无需进行等待，可以立刻地执行下一个分析任务，当任务执行完毕后，用户可以收到通知。

那么想要实现这一点，**需要解决的技术难点在？**

- 如何实现异步化？可以采用协程。
- 任务的分配如何处理？添加到任务队列中，让协程去处理。
- 如何防止多协程拿到同一个任务？对任务队列采用锁。
- 如何控制协程的数量？可以引入协程池。

对于当前的系统，可以采用以下的流程：

1、用户需要进行一次分析，提交了分析请求，分析到达系统中先保存到数据库里，增加一个处理标识。

2、数据库保存成功后，尝试提交这个新任务：

- 任务提交成功：
  - 若存在空闲的协程，那么让协程直接去处理这个任务。
  - 若不存在空闲的协程，那么让任务存放在消息队列中。
- 任务提交失败，例如不存在空闲协程，并且消息队列任务数量已满：
  - **直接拒绝请求**，不再执行。（数据库存放的数据应该被删除）
  - 后台协程检查提交失败的任务，定期将任务取出处理。

3、任务被完成后，更新任务的处理标识，并且通知用户任务已经完成。（带来的问题：多了一次数据库更新）

4、用户可以查询任务的状态。



为了控制协程池的数量，于是决定引入**ants协程池**。

> 那么，ants为我们**解决了什么问题？**
>
> - **可控的并发边界**：可以控制同时并发进行的任务数量，**防止任务过载压垮CPU/内存**。
> - **任务排队**：没有空闲的worker的时候，可以让任务阻塞，等待有空闲的worker去执行。
> - **任务过载控制**：当超过了设置的队列长度后，多出的任务可以**直接返回失败**，进行对任务的标记。

> ants又**带来了什么问题？**
>
> - **系统的维护复杂度上升**
> - **需要注意参数的控制**，例如资源回收，ants在空闲时会回收goroutine，要配置好最大空闲间隔
> - **依赖风险**

参数设置：

- **WithPreAlloc**：设置为4，提前创建好worker进行预热，当首批请求到来无需延迟。
- **WithPanicHandler**：记录panic异常，进行日志打印，可以随时追溯。
- **WithNonblocking**：设置为true，当任务过多，超出20个，就直接拒绝请求，返回错误。否则，会存在大量的协程占用空间，堆积会使得内存爆炸。
- **Size**：设置为4，最大支持4个AI任务并发。
- **WithMaxBlockingTasks**：设置为20，支持20个AI任务阻塞等待执行。

```go
//获取协程池
	aiPool := GetAiGenPool()
	//异步执行
	go func() {
		var err error
		taskErr := aiPool.Submit(func() {
			//修改chart状态为执行中
			chart.Status = consts.ChartStatusRunning
			chart.ExecMessage = "正在执行"
			updateMap := map[string]interface{}{
				"status":       chart.Status,
				"exec_message": chart.ExecMessage,
			}
			err = s.ChartRepo.UpdateChartByMap(nil, chart.ID, updateMap)
			if err != nil {
				//更新失败，返回错误
				return
			}
			//开始处理任务
			//构造AI调用请求参数
			userRequirement := fmt.Sprintf("分析需求:%s", goal)
			if chartType != "" {
				userRequirement += fmt.Sprintf(",图表类型:%s", chartType)
			}
			//调用API
			res, err := siliconflow.NewLLMChatReqeustNoContext(userRequirement, data)
			if err != nil {
				return
			}
			//提取res中的数据
			genChart, genResult, err := s.GetGenResultAndChart(res.Choices[0].Message.Content)
			if err != nil {
				return
			}
			//保存状态
			chart.GenChart = genChart
			chart.GenResult = genResult
			updateMap = map[string]interface{}{
				"status":       consts.ChartStatusSucceed,
				"exec_message": "执行成功",
				"gen_chart":    chart.GenChart,
				"gen_result":   chart.GenResult,
			}
			//存储数据库
			err = s.ChartRepo.UpdateChartByMap(nil, chart.ID, updateMap)
			if err != nil {
				//更新失败，返回错误
				return
			}
		})
		//进行错误处理
		if taskErr != nil {
			//任务提交失败了，进行数据库的更新
			updateMap := map[string]interface{}{
				"status":       consts.ChartStatusFailed,
				"exec_message": "任务提交失败",
			}
			ERR := s.ChartRepo.UpdateChartByMap(nil, chart.ID, updateMap)
			if ERR != nil {
				//进行日志打印
				log.Println("更新任务失败记录失败", ERR)
				return
			}
		}
		if err != nil{
			//AI任务内部执行出错，记录出错信息
			updateMap := map[string]interface{}{
				"status":       consts.ChartStatusFailed,
				"exec_message": err.Error(),
			}
			ERR := s.ChartRepo.UpdateChartByMap(nil, chart.ID, updateMap)
			if ERR != nil {
				//进行日志打印
				log.Println("更新任务失败记录失败", ERR)
				return
			}
		}
	}()
	//提前返回任务ID
	return chart.ID, nil
```

引入了ants后，可以异步执行AI任务，提前返回数据。

# 4、引入协程池后，系统存在的不足分析

引入协程池后，解决了在单机模式下，用户请求被阻塞等待返回造成的体验不佳问题。

但是，目前的系统存在以下的局限：

- 在单机模式下，目前的策略可行。可是系统可扩展性差，假如需要扩展成分布式，那么需要每个系统都配置协程池，以及配置其对任务的处理，**各个实例之间没有统一的请求分发和状态共享，就做不到跨实例的负载均衡和任务迁移。**也就是说，一个实例将任务获取、存储、处理耦合在了一起，难以合理的负载均衡。
- 当流量大的时候，**会造成多个请求积压在一起**，这时候想要缓解，**只能选择拒绝服务，或者阻塞多个执行单元等待处理**。这时候可能会引发服务器的宕机，**OOM危险，对用户的体验也不佳。**
- 任务是存储在内存中，虽然做了持久化在数据库中，但是需要引入额外的子系统去定期处理处理失败的任务，引入额外的开发。

因此，为了解决或改善这些问题，可以引入一个消息队列，做到消息和处理的解耦，在合适的时候，尽可能的让整个系统稳定的获取能处理得过来的任务量。在并发请求高峰期，**消息被暂存到中间件中，可以减轻服务器的处理压力，也引入了分布式的优化思想**。

---

那么引入RabbitMQ，解决了什么问题呢？

- **异步解耦，提升系统的可用性**：执行时间长的任务，无需都阻塞在服务器等待执行，而是在后端系统稳定的获取任务，批量执行，提前返回。
- **平滑流量峰值**：在消息队列中，对消息进行排队，**后端只需根据系统的状态，按照固定的并发能力（协程池的大小）进行消费，无需提升系统的瞬时并发能力。**
- **可靠的消息传递与持久化**：通过消息的确认机制，来保证消息的可靠传输，让消息不会丢失、正确处理；以及单独将消息存储在消息队列系统中，就算服务器宕机也能有效的恢复数据。
- **弹性伸缩**：不同的消费者组可以选择并行消息同一个队列的信息，也可以交换器的路由来并发消费多个队列的信息。

RabbitMQ的引入，又带来了什么问题？

- **额外的系统维护成本**：大型的MQ可能需要一个团队专们去维护消息队列系统的稳定运行，**检查消息队列的状态信息、集群的健康度、内存使用情况等。**
- **重复消费与幂等性**：即使有确认机制，也可能因为ACK或者NACK的丢失，导致重复处理消息。因此必须要设计一个幂等性保护，例如引入一个ID，在ACK前先将数据落盘，接收到消息后先检查对应的消息是否被处理了。（是一个复杂的话题，例如接收到消息后，与MQ断开了链接，然后有新的服务端接入了MQ获取了同样的消息，并且服务端B在检查记录是否被更新的时候，A还没有完成更新。这里可以采用**乐观锁更新**的方式去解决，但是也会引来额外的系统开销。）
- **额外的网络IO开销、数据延迟**：因为要走网络Socket通信，就会有额外的序列化开销，网络I/O开销，增加处理数据的时间。

# 5、闭包并发捕获问题，与解决方案（利用管道，又学会了）

场景如下，使用ants的Submit函数提交异步方法的时候，由于Submit是不允许返回值的，那么如果**函数内部出现了业务的错误，该怎么获取这个错误，进行处理呢**？

第一时间想到的，就是在外面设置一个Error进行闭包捕获。本来正打算就这样子了，突然又意识到，**这个Error不就会被多个协程捕获**，然后进行并发修改，就引起了并发冲突了吗？

还真是这样子。那必须要用其他的办法了。一时间还想不出来

突然就想到了，用管道就好了！**管道不就是为了协程间通信**，解决并发冲突的吗？

好主意！只需要闭包捕获这个管道，就可以并发安全了呀，**把错误传给管道就可以了**。

# 6、并发AI调用冲突分析

因为引入了消息队列，会带来额外的并发问题，对此需要有详细的了解和分析。

一些心得：一直在调这个并发问题，思路就是如果某个步骤出现了失败，是不是应该把它放回消息队列呢，然后进行重试。但是放回消息队列的过程又失败了呢？各种情况引发了多个并发问题，调起来非常困难。**然后我想了一下最近抖音的醒图，然后发现一般不管什么原因，任务失败了，都是直接将任务调成失败状态就好了，让用户去重试**。这样子维护成本就很低了，而且相对来说任务失败不太可能。

最终的流程就是这样子：

- 创建了并发量为4，最大阻塞任务数为20的ants协程池
- 创建了每次Qos为20的消息队列
- 后台协程先获取一个消息队列实例ch，接着调用Consume建立消费者连接，然后不断尝试从消息队列获取数据，然后提交给协程池并发处理任务：
  - 从消息队列获取Chart的ID
  - 根据ID获取Chart实例，失败则发送error
  - 若Chart执行状态为执行完毕，则Ack；若为正在执行，则Nack并给丢弃消息，发送error；若Chart为空，图表被删除，Ack
  - 修改Chart状态为执行中，失败则发送error返回，ack
  - 进行AI调用，期间发生错误则发送error返回，ack
  - 从AI生成中提取内容，错误则发送error返回，ack
  - 保存状态为解析成功，发送错误则发送error返回，ack
  - ack
- 后台处理error的协程，对收到的error消息继续处理：
  - 根据error内的ChartId找到Chart，错误则打印日志，无后续处理
  - 修改Chart状态为失败，错误则打印日志，无后续处理

